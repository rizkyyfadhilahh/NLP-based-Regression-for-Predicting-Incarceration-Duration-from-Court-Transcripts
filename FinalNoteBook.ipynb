{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW2ygAPDG6EY"
      },
      "source": [
        "# LG01 - OBJECTIVE QUEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-h_PnqBFSAk"
      },
      "source": [
        "# DEPEDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T06:53:43.629097Z",
          "iopub.status.busy": "2025-09-07T06:53:43.628765Z",
          "iopub.status.idle": "2025-09-07T06:53:45.478176Z",
          "shell.execute_reply": "2025-09-07T06:53:45.477433Z",
          "shell.execute_reply.started": "2025-09-07T06:53:43.629065Z"
        },
        "id": "opWzNONUFSAm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.sparse import hstack, csr_matrix, save_npz, load_npz\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3euKwBzFm_f"
      },
      "source": [
        "# PREPROCESS DATA & FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T06:53:45.479913Z",
          "iopub.status.busy": "2025-09-07T06:53:45.479497Z",
          "iopub.status.idle": "2025-09-07T07:02:59.791057Z",
          "shell.execute_reply": "2025-09-07T07:02:59.790176Z",
          "shell.execute_reply.started": "2025-09-07T06:53:45.479889Z"
        },
        "id": "8Ci2xMrMFSAn",
        "outputId": "65f33f19-5195-4af2-9d14-bc1f1eff40c3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving preprocessed data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.read_csv(\"/kaggle/input/dataset/train.csv\")\n",
        "df_test = pd.read_csv(\"/kaggle/input/dataset/test.csv\")\n",
        "\n",
        "base_dir = Path('/kaggle/input/dataset/file_putusan/file_putusan')\n",
        "if not base_dir.exists():\n",
        "    base_dir = Path('/kaggle/input/dataset/file_putusan')\n",
        "    if not base_dir.exists():\n",
        "        import os\n",
        "        for root, dirs, files in os.walk('/kaggle/input/dataset/'):\n",
        "            if any('.txt' in f for f in files):\n",
        "                base_dir = Path(root)\n",
        "                break\n",
        "\n",
        "txt_files = list(base_dir.rglob(\"*.txt\"))\n",
        "\n",
        "text_dict = {}\n",
        "for i, path in enumerate(txt_files):\n",
        "    try:\n",
        "        file_id = path.name.replace('.txt', '')\n",
        "        content = path.read_text(encoding='utf-8', errors='ignore')\n",
        "        text_dict[file_id] = content\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "text_df = pd.DataFrame(list(text_dict.items()), columns=['id', 'content'])\n",
        "del text_dict\n",
        "gc.collect()\n",
        "\n",
        "for df in [df_train, df_test, text_df]:\n",
        "    df['id'] = df['id'].astype(str)\n",
        "\n",
        "df_train = pd.merge(df_train, text_df, on='id', how='left')\n",
        "df_test = pd.merge(df_test, text_df, on='id', how='left')\n",
        "df_train['content'] = df_train['content'].fillna('')\n",
        "df_test['content'] = df_test['content'].fillna('')\n",
        "del text_df\n",
        "gc.collect()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text) or text == '':\n",
        "        return ''\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_train['clean_text'] = df_train['content'].apply(preprocess_text)\n",
        "df_test['clean_text'] = df_test['content'].apply(preprocess_text)\n",
        "df_train.drop('content', axis=1, inplace=True)\n",
        "df_test.drop('content', axis=1, inplace=True)\n",
        "gc.collect()\n",
        "\n",
        "all_text = pd.concat([df_train['clean_text'], df_test['clean_text']], ignore_index=True)\n",
        "\n",
        "print(\"Saving preprocessed data...\")\n",
        "df_train.to_csv('train_preprocessed.csv', index=False)\n",
        "df_test.to_csv('test_preprocessed.csv', index=False)\n",
        "all_text.to_csv('all_text.csv', index=False)\n",
        "\n",
        "del df_train, df_test, all_text\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yph4X0Y_HeS8"
      },
      "source": [
        "# TEXT VECTORIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T07:14:07.723041Z",
          "iopub.status.busy": "2025-09-07T07:14:07.721673Z",
          "iopub.status.idle": "2025-09-07T07:25:50.965810Z",
          "shell.execute_reply": "2025-09-07T07:25:50.964925Z",
          "shell.execute_reply.started": "2025-09-07T07:14:07.723004Z"
        },
        "id": "4niwmWzDFSAo",
        "outputId": "7ae314e8-2246-48fe-caff-79091694094e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1: 1500 features saved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_text_df = pd.read_csv('all_text.csv')\n",
        "all_text = all_text_df.iloc[:, 0].tolist()\n",
        "df_train = pd.read_csv('train_preprocessed.csv')\n",
        "df_test = pd.read_csv('test_preprocessed.csv')\n",
        "del all_text_df\n",
        "\n",
        "vec1 = TfidfVectorizer(ngram_range=(1, 2), max_features=1500, min_df=2, max_df=0.95, sublinear_tf=True)\n",
        "vec1.fit(all_text)\n",
        "train_feat1 = vec1.transform(df_train['clean_text'])\n",
        "test_feat1 = vec1.transform(df_test['clean_text'])\n",
        "\n",
        "save_npz('train_feat1.npz', train_feat1)\n",
        "save_npz('test_feat1.npz', test_feat1)\n",
        "print(f\"Batch 1: {train_feat1.shape[1]} features saved\")\n",
        "\n",
        "del vec1, train_feat1, test_feat1\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T07:47:30.488055Z",
          "iopub.status.busy": "2025-09-07T07:47:30.487838Z",
          "iopub.status.idle": "2025-09-07T08:10:25.003498Z",
          "shell.execute_reply": "2025-09-07T08:10:25.002597Z",
          "shell.execute_reply.started": "2025-09-07T07:47:30.488038Z"
        },
        "id": "FAbog-jMFSAo",
        "outputId": "4a02299b-4be1-44ad-818b-ef5c90cf7111",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 2: 1200 features saved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec2 = TfidfVectorizer(ngram_range=(1, 3), max_features=1200, min_df=2, max_df=0.90, sublinear_tf=True)\n",
        "vec2.fit(all_text)\n",
        "train_feat2 = vec2.transform(df_train['clean_text'])\n",
        "test_feat2 = vec2.transform(df_test['clean_text'])\n",
        "\n",
        "save_npz('train_feat2.npz', train_feat2)\n",
        "save_npz('test_feat2.npz', test_feat2)\n",
        "print(f\"Batch 2: {train_feat2.shape[1]} features saved\")\n",
        "\n",
        "del vec2, train_feat2, test_feat2\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T08:10:25.005920Z",
          "iopub.status.busy": "2025-09-07T08:10:25.005671Z",
          "iopub.status.idle": "2025-09-07T08:29:58.865866Z",
          "shell.execute_reply": "2025-09-07T08:29:58.865191Z",
          "shell.execute_reply.started": "2025-09-07T08:10:25.005901Z"
        },
        "id": "Wk8GB3DLFSAo",
        "outputId": "81209e49-8a97-4e00-b9f6-2f7eb9c3c168",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 3: 1000 features saved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec3 = TfidfVectorizer(ngram_range=(2, 3), max_features=1000, min_df=2, max_df=0.85, sublinear_tf=True)\n",
        "vec3.fit(all_text)\n",
        "train_feat3 = vec3.transform(df_train['clean_text'])\n",
        "test_feat3 = vec3.transform(df_test['clean_text'])\n",
        "\n",
        "save_npz('train_feat3.npz', train_feat3)\n",
        "save_npz('test_feat3.npz', test_feat3)\n",
        "print(f\"Batch 3: {train_feat3.shape[1]} features saved\")\n",
        "\n",
        "del vec3, train_feat3, test_feat3\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T08:29:58.866901Z",
          "iopub.status.busy": "2025-09-07T08:29:58.866695Z",
          "iopub.status.idle": "2025-09-07T09:35:16.714860Z",
          "shell.execute_reply": "2025-09-07T09:35:16.713918Z",
          "shell.execute_reply.started": "2025-09-07T08:29:58.866883Z"
        },
        "id": "tMRbonfcFSAp",
        "outputId": "cc03e0a4-b22d-4ea7-ce18-f0a7ca594041",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 4: 800 features saved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec4 = TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=800, min_df=2, sublinear_tf=True)\n",
        "vec4.fit(all_text)\n",
        "train_feat4 = vec4.transform(df_train['clean_text'])\n",
        "test_feat4 = vec4.transform(df_test['clean_text'])\n",
        "\n",
        "save_npz('train_feat4.npz', train_feat4)\n",
        "save_npz('test_feat4.npz', test_feat4)\n",
        "print(f\"Batch 4: {train_feat4.shape[1]} features saved\")\n",
        "\n",
        "del vec4, train_feat4, test_feat4\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T09:35:16.716215Z",
          "iopub.status.busy": "2025-09-07T09:35:16.715810Z",
          "iopub.status.idle": "2025-09-07T10:57:23.292329Z",
          "shell.execute_reply": "2025-09-07T10:57:23.291420Z",
          "shell.execute_reply.started": "2025-09-07T09:35:16.716188Z"
        },
        "id": "yEKWuzw0FSAp",
        "outputId": "db3e64a6-7178-47d0-d218-30d9b6c762e3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: 600 features saved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec5 = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=600, min_df=2, sublinear_tf=True)\n",
        "vec5.fit(all_text)\n",
        "train_feat5 = vec5.transform(df_train['clean_text'])\n",
        "test_feat5 = vec5.transform(df_test['clean_text'])\n",
        "\n",
        "save_npz('train_feat5.npz', train_feat5)\n",
        "save_npz('test_feat5.npz', test_feat5)\n",
        "print(f\"Batch 5: {train_feat5.shape[1]} features saved\")\n",
        "\n",
        "del vec5, train_feat5, test_feat5\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T10:57:23.293893Z",
          "iopub.status.busy": "2025-09-07T10:57:23.293599Z",
          "iopub.status.idle": "2025-09-07T11:09:09.299981Z",
          "shell.execute_reply": "2025-09-07T11:09:09.299164Z",
          "shell.execute_reply.started": "2025-09-07T10:57:23.293866Z"
        },
        "id": "sLvXvXETFSAp",
        "outputId": "53e0012c-870b-4135-a60c-b9f04e336126",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 6: 400 features saved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec6 = CountVectorizer(ngram_range=(1, 2), max_features=400, min_df=2)\n",
        "vec6.fit(all_text)\n",
        "train_feat6 = vec6.transform(df_train['clean_text'])\n",
        "test_feat6 = vec6.transform(df_test['clean_text'])\n",
        "\n",
        "save_npz('train_feat6.npz', train_feat6)\n",
        "save_npz('test_feat6.npz', test_feat6)\n",
        "print(f\"Batch 6: {train_feat6.shape[1]} features saved\")\n",
        "\n",
        "del vec6, train_feat6, test_feat6, all_text\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T11:09:09.301349Z",
          "iopub.status.busy": "2025-09-07T11:09:09.300998Z",
          "iopub.status.idle": "2025-09-07T11:09:16.247380Z",
          "shell.execute_reply": "2025-09-07T11:09:16.246602Z",
          "shell.execute_reply.started": "2025-09-07T11:09:09.301325Z"
        },
        "id": "50YJFsMIFSAq",
        "outputId": "a6d978a8-43b8-4301-f1c8-f344430544d9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TOTAL FEATURES: 5500\n",
            "Train shape: (16572, 5500)\n",
            "Test shape: (6666, 5500)\n"
          ]
        }
      ],
      "source": [
        "train_feat1 = load_npz('train_feat1.npz')\n",
        "train_feat2 = load_npz('train_feat2.npz')\n",
        "train_feat3 = load_npz('train_feat3.npz')\n",
        "train_feat4 = load_npz('train_feat4.npz')\n",
        "train_feat5 = load_npz('train_feat5.npz')\n",
        "train_feat6 = load_npz('train_feat6.npz')\n",
        "\n",
        "test_feat1 = load_npz('test_feat1.npz')\n",
        "test_feat2 = load_npz('test_feat2.npz')\n",
        "test_feat3 = load_npz('test_feat3.npz')\n",
        "test_feat4 = load_npz('test_feat4.npz')\n",
        "test_feat5 = load_npz('test_feat5.npz')\n",
        "test_feat6 = load_npz('test_feat6.npz')\n",
        "\n",
        "X_train_combined = hstack([train_feat1, train_feat2, train_feat3, train_feat4, train_feat5, train_feat6]).tocsr()\n",
        "X_test_combined = hstack([test_feat1, test_feat2, test_feat3, test_feat4, test_feat5, test_feat6]).tocsr()\n",
        "\n",
        "del train_feat1, train_feat2, train_feat3, train_feat4, train_feat5, train_feat6\n",
        "del test_feat1, test_feat2, test_feat3, test_feat4, test_feat5, test_feat6\n",
        "gc.collect()\n",
        "\n",
        "print(f\"TOTAL FEATURES: {X_train_combined.shape[1]}\")\n",
        "print(f\"Train shape: {X_train_combined.shape}\")\n",
        "print(f\"Test shape: {X_test_combined.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8psBH2uZF2Hw"
      },
      "source": [
        "# MODELLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T11:09:16.249066Z",
          "iopub.status.busy": "2025-09-07T11:09:16.248776Z",
          "iopub.status.idle": "2025-09-07T11:26:29.098981Z",
          "shell.execute_reply": "2025-09-07T11:26:29.097936Z",
          "shell.execute_reply.started": "2025-09-07T11:09:16.249037Z"
        },
        "id": "hGBi1wSHFSAq",
        "outputId": "95284f77-7755-43c3-edb7-ca9f98f7a119",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-rmse:26.54027\tvalidation_1-rmse:26.16418\n",
            "[50]\tvalidation_0-rmse:11.15336\tvalidation_1-rmse:14.19617\n",
            "[100]\tvalidation_0-rmse:9.08741\tvalidation_1-rmse:13.86788\n",
            "[150]\tvalidation_0-rmse:7.71623\tvalidation_1-rmse:13.76963\n",
            "[200]\tvalidation_0-rmse:6.61529\tvalidation_1-rmse:13.69654\n",
            "[250]\tvalidation_0-rmse:5.75196\tvalidation_1-rmse:13.65877\n",
            "[300]\tvalidation_0-rmse:5.04600\tvalidation_1-rmse:13.61157\n",
            "[350]\tvalidation_0-rmse:4.46132\tvalidation_1-rmse:13.58084\n",
            "[400]\tvalidation_0-rmse:3.91379\tvalidation_1-rmse:13.56921\n",
            "[450]\tvalidation_0-rmse:3.45484\tvalidation_1-rmse:13.55642\n",
            "[500]\tvalidation_0-rmse:3.04643\tvalidation_1-rmse:13.54896\n",
            "[550]\tvalidation_0-rmse:2.70720\tvalidation_1-rmse:13.53450\n",
            "[599]\tvalidation_0-rmse:2.42685\tvalidation_1-rmse:13.53582\n",
            "VALIDATION RMSE: 13.5308\n"
          ]
        }
      ],
      "source": [
        "y_train = df_train['lama hukuman (bulan)'].values\n",
        "outlier_threshold = np.percentile(y_train, 95)\n",
        "outlier_mask = y_train <= outlier_threshold\n",
        "\n",
        "X_train_clean = X_train_combined[outlier_mask]\n",
        "y_train_clean = y_train[outlier_mask]\n",
        "\n",
        "X_train, X_val, y_train_split, y_val = train_test_split(\n",
        "    X_train_clean, y_train_clean, test_size=0.20, random_state=42\n",
        ")\n",
        "\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.0,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    tree_method='hist',\n",
        "    early_stopping_rounds=80\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    X_train, y_train_split,\n",
        "    eval_set=[(X_train, y_train_split), (X_val, y_val)],\n",
        "    eval_metric='rmse',\n",
        "    verbose=50\n",
        ")\n",
        "\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f\"VALIDATION RMSE: {val_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O7E2kL8Hy9H"
      },
      "source": [
        "## TEST PREDICTION AND MAKING SUBMISSION FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T11:26:29.102144Z",
          "iopub.status.busy": "2025-09-07T11:26:29.101795Z",
          "iopub.status.idle": "2025-09-07T11:26:29.318057Z",
          "shell.execute_reply": "2025-09-07T11:26:29.317440Z",
          "shell.execute_reply.started": "2025-09-07T11:26:29.102104Z"
        },
        "id": "t11shv5-FSAr",
        "outputId": "74889b34-bceb-4cc0-8cf7-183060791477",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUBMISSION CREATED!\n",
            "Final RMSE: 13.5308\n"
          ]
        }
      ],
      "source": [
        "test_predictions = xgb_model.predict(X_test_combined)\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': df_test['id'],\n",
        "    'lama hukuman (bulan)': test_predictions\n",
        "})\n",
        "\n",
        "submission_df.to_csv('finalhopesubmit.csv', index=False)\n",
        "print(\"SUBMISSION CREATED!\")\n",
        "print(f\"Final RMSE: {val_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKY9a_pWGy_P"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICKtxjHVGVeD"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('xgb_model.pkl', 'wb') as f:\n",
        "    pickle.dump(xgb_model, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 8133836,
          "sourceId": 12859588,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
